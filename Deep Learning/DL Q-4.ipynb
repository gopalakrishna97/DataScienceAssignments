{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b724ae7",
   "metadata": {},
   "source": [
    "## Question 4 -\n",
    "Design an end-to-end solution with diagrams for object detection use cases\n",
    "leveraging AWS cloud services and open-source tech\n",
    "Note -\n",
    "1. You need to use both AWS cloud services and open-source tech to design the\n",
    "entire solution\n",
    "2. The pipeline should consist of a data pipeline, ml pipeline, deployment pipeline,\n",
    "and inference pipeline.\n",
    "3. In the data pipeline, you would be designing how to get the data from external or\n",
    "existing sources and tech used for the same\n",
    "4. In the ml pipeline, you would be designing how to train the model, and what all\n",
    "algorithms, techniques, etc. would you be using. Again, tech used for the same \n",
    "5. Since this is a deep learning project, the use of GPUs, and how effectively are you\n",
    "using them to optimize for cost and training time should also be taken into\n",
    "consideration.\n",
    "6. In the deployment pipeline, you would be designing how effectively and\n",
    "efficiently you are deploying the model in the cloud,\n",
    "7. In the inference pipeline, consider the cost of inference and its optimization\n",
    "related to computing resources and handling external traffic\n",
    "8. You can use any tool to design the architecture\n",
    "9. Do mention the pros and cons of your architecture and how much further it can\n",
    "be optimized and its tradeoffs.\n",
    "10. Do include a retraining approach as well.\n",
    "11. Try to include managed AWS resources for deep learning like AWS Textract,\n",
    "AWS Sagemaker, etc., and not just general-purpose compute resources like S3,\n",
    "EC2, etc. Try to mix the best of both services\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e5d9f",
   "metadata": {},
   "source": [
    "                 +-----------------+\n",
    "                 |    Data Source   |\n",
    "                 +--------+--------+\n",
    "                          |\n",
    "                          |\n",
    "                          v\n",
    "                 +-----------------+\n",
    "                 |   Data Pipeline |\n",
    "                 +-----------------+\n",
    "                          |\n",
    "                          |\n",
    "                          v\n",
    "        +--------------------------------+\n",
    "        |         Preprocessing          |\n",
    "        +--------------------------------+\n",
    "                          |\n",
    "                          |\n",
    "                          v\n",
    "        +--------------------------------+\n",
    "        |        Model Training          |\n",
    "        +--------------------------------+\n",
    "                          |\n",
    "                          |\n",
    "                          v\n",
    "        +--------------------------------+\n",
    "        |        Model Deployment        |\n",
    "        +--------------------------------+\n",
    "                          |\n",
    "                          |\n",
    "                          v\n",
    "        +--------------------------------+\n",
    "        |         Inference Pipeline      |\n",
    "        +--------------------------------+\n",
    "                          |\n",
    "                          |\n",
    "                          v\n",
    "        +--------------------------------+\n",
    "        |        Post-processing          |\n",
    "        +--------------------------------+\n",
    "                          |\n",
    "                          |\n",
    "                          v\n",
    "        +--------------------------------+\n",
    "        |       Output Visualization     |\n",
    "        +--------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e210fdc5",
   "metadata": {},
   "source": [
    "This diagram represents the different components and pipelines involved in the solution:\n",
    "1. Data Pipeline: Handles data ingestion from various sources and transfers it to the preprocessing stage.\n",
    "2. Preprocessing: Performs data preprocessing tasks such as resizing, normalization, and augmentation.\n",
    "3. Model Training: Trains the object detection model using deep learning algorithms and frameworks. Utilizes GPU instances for accelerated training.\n",
    "4. Model Deployment: Deploys the trained model using AWS services like AWS Lambda, AWS Fargate, or AWS EC2 instances based on scale and latency requirements.\n",
    "5. Inference Pipeline: Handles real-time inference requests by preprocessing incoming data and passing it through the deployed object detection model.\n",
    "6. Post-processing: Applies post-processing techniques like filtering, non-maximum suppression, or bounding box refinement to improve detection accuracy.\n",
    "7. Output Visualization: Visualizes the object detection results, such as bounding boxes and labels, for user interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac1705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
